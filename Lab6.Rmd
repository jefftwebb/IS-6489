---
title: "Statistics and Predictive Analytics, Lab 6"
author: "Your name here"
output:
  html_notebook
---

Last lab!

In lecture we talked about ROC curves, class decision thresholds and introduced SVMs. This lab will give you practice working with those techniques.  Here are the topics we will cover:

- Data exploration and cleaning.
- Probability and conditional probability of an event.
- Logistic regression models.
- KNN and SVM classification.
- Area under the curve (AUC).
- Using logistic regression to calculate change in the probability of an event.


```{r setup, include=F}
knitr::opts_chunk$set(echo = T, include=T, 
                      warning = F, message =F)
library(tidyverse)
library(caret)
library(arm)
library(MASS) # This includes versions of the Pima dataset

```


Download the "adult" dataset from the folder for Lab 6 in Canvas.  See the data dictionary: https://archive.ics.uci.edu/ml/datasets/adult and  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names. Income will be the outcome variable for the analyses using this dataset.

### Data Exploration and cleaning

Let's start by doing some data exploration in order to assess what sort of cleaning needs to take place.

```{r}
a <- read.csv("adult.csv")[,-1]

glimpse(a)

```

The dataset has 13 variables and 32,561 observations, with many factor variables.  We'll look at a summary of the data to assess whether any cleaning is necessary.

```{r}
summary(a)

```

The ranges of the numeric variables look reasonable, as do the counts of the categorical variables.  Note that some variables contain missing observations denoted  by a question mark ("?").  We could leave those unchanged for the analysis---the question mark would be treated as just another factor level---or we could impute those missing observations, provided that they do not simply indicate an "other" category, or we could remove them. We should inspect the missing observations  to understand them better, but first need to figure out which variables have missing observations.

```{r}

# Create a new dataset for summarizing the missing variables
missings <- data.frame(variables = names(a), missings = NA)

# Double-check to make sure this worked
head(missings)

# Use a loop to fill the missings column with an indicator:
# 1 if any missings, 0 otherwise
for(i in 1:nrow(missings)){
  missings[i, 2] <- ifelse(any(a[,i]=="?"), 1, 0)
}

# Inspect the summary of missings
missings
```

Is there a structure to the missings? Basically, we want to see whether we can discern from the structure of the data how we might impute values for "?." 

1. Occupation

```{r}
summary(a$occupation) # 1843 ?s

# Select only rows where occupation == "?"
subset(a, occupation == "?") %>% 
  head(20)

```

Notice that there is an "other-service" category, but not a general "other" category, so perhaps "?" simply represents "other."  When `occupation` is "?" so is `type_employer`, not surprisingly.

2. Type-employer
```{r}
# Summarize
summary(a$type_employer)
```

No "other" category here either.

3. Country

```{r}
# Summarize
summary(a$country) # 583 ?s

# Select only country == "?"
subset(a, country == "?") %>% 
  head(20)
```

The fact that `country` also has values of "?" suggests that "?" does not represent a general "other" category. Nor is there any discernible logical structure that would allow us to figure out what "?" represents. All three variables with missings are likely to be strong predictors of income---especially the occupation variables---so I think it makes sense to simply remove the rows with question marks, especially given the large number of rows without any missing observations. That way we'll be working with a clean dataset.  In perusing the papers cited in the data dictionary, furthermore, it appears that other researchers have done just that, stripping out the unknowns. 

There are risks to this strategy, of course, because the data are most likely missing at random (MAR).  One way to approach this issue is to conduct a rough sensitivity analysis in which you impute the values with "?," conduct the analysis, and then compare the results with those from the analysis in which the missings have been deleted.  Ideally, they are similar: you don't want want your results to hinge on data modeling decisions.  If they are not similar then you've go a problem.  You'll need to think harder about data modeling.

Do additional data exploration here:

```{r}
# Your code goes here

```

### Probability

**Question 1**.  Remove all the rows with question marks (you should have 30162 rows in the cleaned dataset) and calculate the probability, rounded to 2 decimals, of earning more than 50k in this dataset. In this case the probability is just the proportion of people earning more than 50k.  

```{r}
# Your code goes here

```
> Your answer goes here

After removing the rows with "?" we need to refactor country, occupation and type_employer, otherwise R will retain the now empty category of "?" as the reference category.  

```{r}
a$country <- factor(a$country)
a$occupation <- factor(a$occupation)
a$type_employer <- factor(a$type_employer)
```

**Question 2**:  Workers with occupation listed as "Exec-managerial" have the highest probability in this dataset of having an income greater than 50k.  What is that probability (rounded to two decimals)? 

```{r }
# Write your code here

```

>Your answer goes here.

**Question 3**:  Which occupation has the *lowest* probability of having an income greater than 50k?

```{r}
# Your code goes here

```

> Your answer goes here.

### Logistic regression

Next we will fit a sequence of models of income using caret and its default bootstrap cross-validation.  Make sure to set the seed as indicated, and to center and scale where appropriate.  These are computationally expensive operations so, to speed model fitting, we will prepare a random subset of the data (2000 rows). Allow caret's default settings to pick optimal values for user-specified parameters.

```{r}
set.seed(38)
a_sub <- sample_n(a, 2000)

summary(a_sub)
```

**Question 4**: Fit a logistic model of income, using all the predictors.  Report caret's cross-validation estimate of out-of-sample accuracy for this model. In caret, the method argument for logistic regression is "glm." Remember to use the subsetted data in the data argument.

```{r}
# Your code goes here
set.seed(38)

```

> Your answer goes here.

**Question 5**: Fit a glmnet model of income, using all the predictors.  Glmnet works fine for classification. Report caret's cross-validation estimate of out-of-sample accuracy for this model.

```{r}
# Your code goes here
set.seed(38)

```


You can look at the model coefficients with this code (replacing "glmnet_model" with your own model name):

```{r}
coef(glmnet_model$finalModel, glmnet_model$finalModel$tuneValue$lambda) %>% 
  round(2)
```

> Your answer goes here.

### Classification with KNN and SVM

**Question 6**: Fit a KNN model of income, using all the predictors.  Report caret's cross-validation estimate of out-of-sample accuracy for this model.

```{r}
# Your code goes here
set.seed(38)

```

> Your answer goes here.

**Question 7**: Fit a linear support vector machine (SVM) model of income, using all the predictors.  Report caret's cross-validation estimate of out-of-sample accuracy for this model. Set the method argument to "svmLinear2."  

```{r}
# Your code goes here
set.seed(38)

```

> Your answer goes here.

The Lasso model had the best performance, given this seed, just *barely* beating out SVM. Additionally, Lasso has a big advantage in that it is easy to extract probabilities from it, whereas for machine learning models such as SVM (and KNN) probabilities must be reverse engineered (given the way those algorithms work). 

### AUC

**Question 8**: Calculate AUC for the glmnet model, using the entire dataset in the arguments to the `roc` function.

```{r}
# Your code goes here

```

> Your answer goes here.

### Calculating change in probability 

We can use the lasso model to calculate the probability of having income greater than 50k income. Remember that the inputs to this model are (or should be) centered. So the the intercept, for example, represents the average (or reference case) log odds of greater than 50k earnings: -1.81.

**Question 9**:  Using the glmnet model, calculate the change in probability of earning more than 50k change for someone who is 30 years old vs 50 years old with the following characteristics:
- type_employer = Private
- education_num = 12
- marital = Married-civ-spouse
- occupation = Prof-specialty
- relationship = Husband
- race = White
- sex = Male
- capital_gain = 0
- capital_loss = 0
- hr_per_week = 40
- country = United-States
      

```{r}
# Your code goes here

```

> Your answer goes here

**Question 10 (optional)**: A change of pace.  Download the Pima dataset. 

```{r}
data("Pima.te")
data("Pima.tr")
pima <- rbind(Pima.te, Pima.tr)
```

Fit a model that predicts type using npreg, ped, glu, bmi,  age and an interaction between npreg  and ped (df = 525).

```{r}
# Your code goes here

```


Suppose that a non-profit health organization wanted to use your model to predict diabetes in this population, but they wanted to do so very conservatively.  That is, they aren't worried about **over**predicting diabetes.  (At the worst, they reason, it would be an inconvenience for patients to come in to get tested.) Instead, they want to avoid **under**predicting diabetes.  Specifically, they would like to push the number of false negatives (instances where the model predicts incorrectly that a person does not have diabetes) down to no more than 2% of the population.  That is, if the population were 1000, they would want there to be no more than 20 false negatives. Which class decision threshold would achieve this objective?  

```{r}

# Your code goes here

```

> Your answer goes here

**Question 11**:  Please score yourself on this lab using the answer key.  For each question, reflect on the difficulty of the question for you and whether you:

- Did not try (just copied the answer key or left it blank).
- Looked at the answer key to get a hint.
- Tried but got the wrong answer.
- Tried and got the right answer.

>Write your answers here.

**Next step.**  After you've finished this lab, go take Quiz 6.  You have one try at the quiz, limited to 30 minutes.  There are five multiple choice and multiple answer questions focusing on the material from this lab.

 